# ğŸ“– ReadFlow - End-to-End Book Analytics Platform

A production-style data engineering project built with 100% free, open-source tools. Process real-time book data through a complete data lake and warehouse architecture.

## ğŸ¯ Project Overview

**ReadFlow** is an end-to-end data pipeline that:
- Ingests book data from GoodReads API in real-time
- Stores raw and processed data in a data lake (MinIO)
- Transforms data using Apache Spark (Bronze â†’ Silver â†’ Gold)
- Loads data into a SQL warehouse (DuckDB)
- Provides interactive analytics via Streamlit

## ğŸ—ï¸ Architecture

```
GoodReads API (real-time)
        â†“
Local Landing (JSON)
        â†“
Apache Airflow (10-min schedule)
        â†“
MinIO Data Lake (Parquet)
        â†“
Apache Spark ETL (Bronze â†’ Silver â†’ Gold)
        â†“
DuckDB Warehouse (Star Schema)
        â†“
Streamlit Analytics App
```

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Data Source** | GoodReads API | Book, author, review data |
| **Ingestion** | Python | API client with retry logic |
| **Orchestration** | Apache Airflow | Workflow scheduling |
| **Object Storage** | MinIO | S3-compatible data lake |
| **File Format** | Parquet | Columnar storage |
| **Processing** | Apache Spark | Distributed ETL |
| **Data Quality** | Python + Great Expectations | Validation & checks |
| **Warehouse** | DuckDB | SQL analytics engine |
| **Analytics UI** | Streamlit | Interactive dashboards |
| **Infrastructure** | Docker Compose | Container orchestration |

## ğŸ“ Project Structure

```
readflow/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ goodreads_ingestion_dag.py
â”‚   â”‚   â””â”€â”€ spark_etl_dag.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ config/
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py
â”‚   â”‚   â””â”€â”€ silver_to_gold.py
â”‚   â”œâ”€â”€ transformations/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ api_client.py
â”‚   â”œâ”€â”€ schema_validator.py
â”‚   â””â”€â”€ landing_zone.py
â”œâ”€â”€ data_quality/
â”‚   â”œâ”€â”€ expectations/
â”‚   â””â”€â”€ validation.py
â”œâ”€â”€ warehouse/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ fact_book_ratings.sql
â”‚   â”‚   â””â”€â”€ dim_book.sql
â”‚   â””â”€â”€ queries/
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â”œâ”€â”€ pages/
â”‚   â””â”€â”€ components/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.9+
- 8GB RAM minimum
- GoodReads API Key (free signup)

### 1. Clone and Setup

```bash
git clone <your-repo>
cd readflow

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env and add your GoodReads API key
```

### 3. Start Infrastructure

```bash
# Start MinIO, Airflow, Spark
docker-compose up -d

# Wait for services to be healthy
docker-compose ps
```

### 4. Access Services

- **Airflow UI**: http://localhost:8080 (admin/admin)
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)
- **Streamlit App**: http://localhost:8501
- **Spark UI**: http://localhost:4040 (when jobs running)

### 5. Initialize Data Lake

```bash
python scripts/init_minio_buckets.py
```

### 6. Run the Pipeline

Enable the DAG in Airflow UI or trigger manually:

```bash
airflow dags trigger goodreads_ingestion_pipeline
```

## ğŸ“Š Data Pipeline Details

### Layer 1: Bronze (Raw Data)

**Purpose**: Store raw API responses without transformation

**Location**: `s3://readflow/bronze/goodreads_raw/`

**Format**: JSON (partitioned by ingestion_date)

**Schema**: Exact API response structure

### Layer 2: Silver (Cleaned Data)

**Purpose**: Cleaned, deduplicated, and normalized data

**Location**: `s3://readflow/silver/goodreads_clean/`

**Format**: Parquet (partitioned by ingestion_date, genre)

**Transformations**:
- Flatten nested JSON structures
- Deduplicate records by book_id
- Standardize timestamps (UTC)
- Clean text fields (remove special chars)
- Handle nulls and default values

### Layer 3: Gold (Analytics Models)

**Purpose**: Business-ready star schema for analytics

**Location**: `s3://readflow/gold/goodreads_analytics/`

**Format**: Parquet (partitioned by date, genre)

**Star Schema**:

**Fact Tables**:
- `fact_book_ratings` - Rating events with FK to dimensions
- `fact_reviews` - Review details with metadata

**Dimension Tables**:
- `dim_book` - Book master data
- `dim_author` - Author information
- `dim_genre` - Genre hierarchy
- `dim_time` - Date dimension

## ğŸ”„ Airflow DAG Design

### Main DAG: `goodreads_ingestion_pipeline`

```
fetch_api_data >> validate_schema >> write_to_bronze >> trigger_spark_etl
                                            â†“
                                    data_quality_check
```

**Schedule**: Every 10 minutes

**Features**:
- Retry logic with exponential backoff
- API rate limit handling
- Incremental data fetch
- Idempotent operations

### ETL DAG: `spark_transformation_pipeline`

```
bronze_to_silver >> silver_to_gold >> load_to_warehouse >> refresh_analytics
```

## âš¡ Spark Job Optimization

### Performance Features

1. **Partition Pruning**: Query only relevant partitions
2. **Columnar Scans**: Read only needed columns
3. **Broadcast Joins**: For small dimension tables
4. **Incremental Processing**: Process only new data
5. **Caching**: Cache frequently used DataFrames

### Example Spark Configuration

```python
spark = SparkSession.builder \
    .appName("ReadFlow ETL") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.files.maxPartitionBytes", "128MB") \
    .getOrCreate()
```

## ğŸ§ª Data Quality Framework

### Validation Checks

**Schema Validation**:
- Column presence
- Data type verification
- Required field checks

**Business Rules**:
- Rating range: 1-5
- Non-null book_id
- Valid ISBN format
- Future date prevention

**Statistical Checks**:
- Row count anomalies
- Null percentage thresholds
- Duplicate detection

### Great Expectations Integration

```python
# Example expectation suite
{
    "expect_column_values_to_be_between": {
        "column": "rating",
        "min_value": 1,
        "max_value": 5
    },
    "expect_column_values_to_not_be_null": {
        "column": "book_id"
    }
}
```

## ğŸ“ˆ Analytics Application

### Streamlit Features

1. **Genre Explorer**
   - Top books by genre
   - Rating distributions
   - Popularity trends

2. **Author Dashboard**
   - Publication timeline
   - Average ratings
   - Review counts

3. **Review Analytics**
   - Sentiment proxy (rating-based)
   - Review length analysis
   - Temporal patterns

4. **Search & Discovery**
   - Semantic book search
   - Similar books recommendation
   - Author lookup

5. **Performance Metrics**
   - Pipeline health dashboard
   - Data freshness indicators
   - Quality score trends

## ğŸ¯ Key Concepts Demonstrated

### Data Engineering Principles

âœ… **Medallion Architecture**: Bronze â†’ Silver â†’ Gold layers
âœ… **Idempotency**: Safe to re-run pipelines
âœ… **Incremental Processing**: Process only new data
âœ… **Schema Evolution**: Handle API changes gracefully
âœ… **Data Quality**: First-class validation
âœ… **Partition Strategy**: Optimize for query patterns
âœ… **Separation of Concerns**: Clear layer boundaries

### Production Best Practices

âœ… **Retry Logic**: Handle transient failures
âœ… **Monitoring**: Log all operations
âœ… **Alerting**: Notify on failures
âœ… **Testing**: Unit and integration tests
âœ… **Documentation**: Clear README and comments
âœ… **Configuration Management**: Environment variables
âœ… **CI/CD Ready**: GitHub Actions templates included

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/unit/

# Run integration tests
pytest tests/integration/

# Validate Airflow DAGs
python -m pytest tests/dags/

# Check data quality
python scripts/run_quality_checks.py
```

## ğŸ“¦ Deployment Options

### Option 1: Local Development (Current)
- Docker Compose on laptop
- Perfect for development and demos

### Option 2: Cloud Migration (Future)
- MinIO â†’ AWS S3
- Local Spark â†’ EMR/Databricks
- DuckDB â†’ Redshift/BigQuery
- **Same code works!** Just change configs

### Option 3: Kubernetes (Advanced)
- Helm charts for all services
- Auto-scaling Spark executors
- Production-grade monitoring

## ğŸ¤ Interview Talking Points

### When asked about this project:

**"What does it do?"**
> "ReadFlow is an end-to-end data platform that ingests book data from GoodReads API, transforms it through a medallion architecture data lake, and serves analytics through a SQL warehouse and interactive dashboard."

**"What's impressive about it?"**
> "It demonstrates production patterns: orchestrated workflows, data quality checks, incremental processing, star schema modeling, and performance optimization - all using industry-standard tools like Airflow, Spark, and Parquet."

**"What challenges did you solve?"**
> "API rate limiting with retry logic, schema evolution handling, incremental vs full loads, partition strategy for query performance, and building idempotent pipelines that are safe to re-run."

**"How does it scale?"**
> "The architecture is cloud-portable. MinIO uses the S3 API, so migrating to AWS S3 is a config change. Spark jobs use partitioning and columnar formats for efficiency. The medallion architecture separates hot and cold data."

## ğŸ“š Learning Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Best Practices](https://spark.apache.org/docs/latest/)
- [Data Lake Design Patterns](https://www.databricks.com/glossary/data-lakehouse)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)
- [DuckDB Analytics](https://duckdb.org/docs/)

## ğŸ¤ Contributing

This is a portfolio project, but feedback welcome! Open an issue or PR.

## ğŸ“„ License

MIT License - Feel free to use for your portfolio

## ğŸ”¥ Resume Description

**ReadFlow â€” End-to-End Book Analytics Platform**

Built an end-to-end data pipeline processing real-time GoodReads API data for book and review analytics. Designed a data lake and warehouse architecture using S3-compatible object storage and Parquet datasets. Orchestrated Spark ETL jobs with Apache Airflow, scheduled at 10-minute intervals. Implemented data cleaning, deduplication, and star-schema modeling for analytics workloads. Enabled SQL-based analytics using DuckDB and delivered insights via an interactive Streamlit application. Applied data quality checks and performance optimizations to ensure reliable and scalable analytics.

---

**Built with â¤ï¸ using 100% free, open-source tools**

Total cost: **â‚¹0** | Production vibes: **100%**
